{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb6a2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.192 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/githome/Object_detection/Num.yolov8/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=80, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train17, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train17, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=11\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753457  ultralytics.nn.modules.head.Detect           [11, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,012,993 parameters, 3,012,977 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 188.074.0 MB/s, size: 15.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\githome\\Object_detection\\Num.yolov8\\train\\labels.cache... 213 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 213/213  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 59.329.7 MB/s, size: 10.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\githome\\Object_detection\\Num.yolov8\\valid\\labels.cache... 20 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 20/20 20034.9it/s 0.0s\n",
      "Plotting labels to runs\\detect\\train17\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train17\u001b[0m\n",
      "Starting training for 80 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/80      2.16G      2.469      4.458      1.704         68        640: 100% ━━━━━━━━━━━━ 14/14 6.1it/s 2.3s0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 6.4it/s 0.2s\n",
      "                   all         20        118      0.016       0.43     0.0504     0.0282\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/80      2.17G      1.651      3.983       1.24         39        640: 100% ━━━━━━━━━━━━ 14/14 9.7it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 9.5it/s 0.1s\n",
      "                   all         20        118     0.0124      0.528     0.0842     0.0456\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/80      2.19G      1.613      3.307      1.242         22        640: 100% ━━━━━━━━━━━━ 14/14 9.7it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 9.7it/s 0.1s\n",
      "                   all         20        118     0.0142      0.696      0.122     0.0656\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/80      2.21G      1.592      2.962      1.247         38        640: 100% ━━━━━━━━━━━━ 14/14 9.6it/s 1.5s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 8.8it/s 0.1s\n",
      "                   all         20        118     0.0187      0.917       0.16     0.0764\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/80      2.21G       1.63      2.765      1.266         54        640: 100% ━━━━━━━━━━━━ 14/14 8.0it/s 1.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.1it/s 0.1s\n",
      "                   all         20        118      0.553     0.0651      0.214      0.101\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/80      2.21G      1.576      2.673      1.265         47        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.9it/s 0.1s\n",
      "                   all         20        118      0.335      0.196      0.198     0.0714\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/80      2.21G      1.535      2.562      1.264         35        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.6it/s 0.1s\n",
      "                   all         20        118      0.329      0.181      0.267      0.122\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/80      2.21G        1.5      2.459      1.234         42        640: 100% ━━━━━━━━━━━━ 14/14 9.8it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.1it/s 0.1s\n",
      "                   all         20        118      0.337      0.269      0.277       0.14\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/80      2.21G      1.533      2.464      1.282         18        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.3it/s 0.1s\n",
      "                   all         20        118      0.372      0.313      0.313      0.129\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/80      2.21G      1.482      2.342      1.236         39        640: 100% ━━━━━━━━━━━━ 14/14 9.8it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.8it/s 0.1s\n",
      "                   all         20        118      0.445      0.488      0.454      0.236\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/80      2.22G      1.481      2.278      1.248         34        640: 100% ━━━━━━━━━━━━ 14/14 10.1it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.1it/s 0.1s\n",
      "                   all         20        118       0.56      0.493      0.489      0.256\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/80      2.23G      1.457      2.238      1.194         68        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.5it/s 0.1s\n",
      "                   all         20        118      0.524      0.486      0.465      0.207\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/80      2.25G      1.432       2.12      1.219         59        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.5it/s 0.1s\n",
      "                   all         20        118      0.552      0.589      0.517      0.246\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/80      2.27G      1.431      2.054      1.213         46        640: 100% ━━━━━━━━━━━━ 14/14 9.7it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 9.8it/s 0.1s\n",
      "                   all         20        118      0.568      0.485      0.532      0.234\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/80      2.27G      1.461      2.073      1.227         33        640: 100% ━━━━━━━━━━━━ 14/14 9.3it/s 1.5s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.5it/s 0.1s\n",
      "                   all         20        118      0.648      0.517      0.575       0.28\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/80      2.27G      1.402      1.967      1.209         33        640: 100% ━━━━━━━━━━━━ 14/14 9.7it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.2it/s 0.1s\n",
      "                   all         20        118      0.566      0.634      0.653      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/80      2.27G      1.474      2.033      1.248         92        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.5it/s 0.1s\n",
      "                   all         20        118      0.633      0.588      0.623      0.339\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/80      2.27G      1.424      1.942      1.206         43        640: 100% ━━━━━━━━━━━━ 14/14 9.8it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.4it/s 0.1s\n",
      "                   all         20        118      0.647       0.67      0.687      0.375\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/80      2.27G      1.391      1.952      1.206         28        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.5it/s 0.1s\n",
      "                   all         20        118      0.642      0.565       0.63      0.341\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/80      2.27G      1.349      1.858      1.197         38        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.4it/s 0.1s\n",
      "                   all         20        118      0.663      0.664      0.702      0.364\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/80      2.27G      1.372      1.743      1.176         66        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4ss.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.6it/s 0.1s\n",
      "                   all         20        118      0.691      0.649      0.746      0.396\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/80      2.27G      1.359      1.787      1.179         72        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.3it/s 0.1s\n",
      "                   all         20        118       0.72      0.677      0.721      0.424\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/80      2.27G      1.359      1.783      1.202         47        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.8it/s 0.1s\n",
      "                   all         20        118      0.684      0.612      0.712       0.33\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/80      2.27G      1.399        1.8      1.195         40        640: 100% ━━━━━━━━━━━━ 14/14 9.8it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.9it/s 0.1s\n",
      "                   all         20        118      0.668       0.63       0.69      0.385\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/80      2.27G      1.357      1.754      1.208         36        640: 100% ━━━━━━━━━━━━ 14/14 10.1it/s 1.4s.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.2it/s 0.1s\n",
      "                   all         20        118      0.728      0.677       0.73      0.423\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/80      2.27G      1.329      1.707      1.187         59        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4ss.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.0it/s 0.1s\n",
      "                   all         20        118      0.731       0.59      0.682      0.319\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/80      2.27G      1.363      1.675      1.175         36        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.3it/s 0.1s\n",
      "                   all         20        118      0.698      0.651      0.734      0.414\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/80      2.27G      1.322      1.636      1.186         54        640: 100% ━━━━━━━━━━━━ 14/14 9.8it/s 1.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.4it/s 0.1s\n",
      "                   all         20        118      0.763       0.66      0.752      0.406\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/80      2.27G      1.301      1.609      1.157         20        640: 100% ━━━━━━━━━━━━ 14/14 10.1it/s 1.4s.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.8it/s 0.1s\n",
      "                   all         20        118       0.71      0.713      0.747      0.426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/80      2.27G      1.318      1.637      1.184         14        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.0it/s 0.1s\n",
      "                   all         20        118      0.763      0.728      0.777      0.448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      31/80      2.27G      1.333      1.619      1.177         53        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.0it/s 0.1s\n",
      "                   all         20        118      0.806       0.71      0.771      0.423\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      32/80      2.27G      1.336       1.61      1.168         43        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.5it/s 0.1s\n",
      "                   all         20        118      0.724       0.63       0.73      0.405\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      33/80      2.27G      1.314      1.613      1.154         55        640: 100% ━━━━━━━━━━━━ 14/14 9.8it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.4it/s 0.1s\n",
      "                   all         20        118      0.663      0.595      0.669      0.321\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      34/80      2.29G      1.327      1.596      1.162         41        640: 100% ━━━━━━━━━━━━ 14/14 9.8it/s 1.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.1it/s 0.1s\n",
      "                   all         20        118       0.82      0.668      0.763      0.425\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      35/80      2.29G      1.298        1.5      1.135         40        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.4it/s 0.1s\n",
      "                   all         20        118      0.761      0.731      0.728        0.4\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      36/80      2.29G      1.318      1.497       1.17         52        640: 100% ━━━━━━━━━━━━ 14/14 9.8it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.3it/s 0.1s\n",
      "                   all         20        118       0.75      0.694      0.734      0.394\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      37/80      2.29G      1.328      1.529      1.163         43        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.8it/s 0.1s\n",
      "                   all         20        118       0.73      0.622      0.691      0.379\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      38/80      2.29G      1.298      1.502      1.143         68        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.0it/s 0.1s\n",
      "                   all         20        118       0.78      0.668      0.747       0.42\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      39/80      2.29G      1.269      1.515      1.154         23        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.5it/s 0.1s\n",
      "                   all         20        118      0.802      0.743      0.781      0.418\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      40/80      2.29G      1.268      1.469      1.142         48        640: 100% ━━━━━━━━━━━━ 14/14 9.7it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.9it/s 0.1s\n",
      "                   all         20        118      0.821      0.695      0.772      0.432\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      41/80      2.29G      1.269      1.414      1.126         52        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.5it/s 0.1s\n",
      "                   all         20        118      0.774       0.69      0.754      0.421\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      42/80      2.29G      1.306      1.442      1.166         41        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.2it/s 0.1s\n",
      "                   all         20        118       0.77      0.623       0.74      0.425\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      43/80      2.29G       1.29      1.493      1.153         28        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.4it/s 0.1s\n",
      "                   all         20        118      0.698        0.7      0.706      0.366\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      44/80      2.29G      1.275      1.451      1.137         41        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.2it/s 0.1s\n",
      "                   all         20        118      0.847       0.68      0.748      0.422\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      45/80      2.29G      1.243       1.42      1.146         42        640: 100% ━━━━━━━━━━━━ 14/14 9.8it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.4it/s 0.1s\n",
      "                   all         20        118      0.801      0.728      0.755      0.412\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      46/80      2.29G      1.283      1.421      1.155         49        640: 100% ━━━━━━━━━━━━ 14/14 9.8it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.4it/s 0.1s\n",
      "                   all         20        118      0.743      0.755      0.743      0.423\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      47/80      2.29G      1.268      1.384      1.142         37        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.6it/s 0.1s\n",
      "                   all         20        118      0.767      0.681      0.737      0.386\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      48/80      2.29G      1.251      1.353       1.12         43        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.1it/s 0.1s\n",
      "                   all         20        118      0.757      0.745      0.775      0.448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      49/80      2.29G      1.261      1.377      1.143         52        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.4it/s 0.1s\n",
      "                   all         20        118      0.808      0.716      0.764      0.433\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      50/80      2.29G      1.239      1.299      1.119         36        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.5it/s 0.1s\n",
      "                   all         20        118      0.805      0.693      0.767      0.423\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      51/80      2.29G      1.245      1.318       1.13         65        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.4it/s 0.1s\n",
      "                   all         20        118      0.834      0.682      0.766      0.436\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      52/80      2.29G      1.257      1.323      1.139         42        640: 100% ━━━━━━━━━━━━ 14/14 9.7it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.6it/s 0.1s\n",
      "                   all         20        118      0.848      0.745      0.779      0.458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      53/80      2.29G      1.267      1.328      1.141         28        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.4it/s 0.1s\n",
      "                   all         20        118      0.856      0.685      0.763      0.451\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      54/80      2.29G      1.226      1.284      1.125         33        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.4it/s 0.1s\n",
      "                   all         20        118      0.749      0.694      0.734      0.396\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      55/80      2.29G      1.221      1.294      1.113         51        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.6it/s 0.1s\n",
      "                   all         20        118      0.762       0.75      0.769      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      56/80      2.29G      1.202      1.251       1.12         35        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.2it/s 0.1s\n",
      "                   all         20        118      0.838      0.743       0.78      0.459\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      57/80      2.29G      1.251      1.251       1.12         46        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 8.7it/s 0.1s\n",
      "                   all         20        118      0.799      0.739      0.776       0.45\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      58/80      2.29G      1.221      1.238      1.098         70        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4ss.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.6it/s 0.1s\n",
      "                   all         20        118      0.839      0.679      0.767      0.452\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      59/80      2.29G      1.208      1.226      1.118         39        640: 100% ━━━━━━━━━━━━ 14/14 10.1it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.4it/s 0.1s\n",
      "                   all         20        118      0.773      0.668       0.75      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      60/80      2.29G      1.226      1.245      1.109         35        640: 100% ━━━━━━━━━━━━ 14/14 10.1it/s 1.4s.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.6it/s 0.1s\n",
      "                   all         20        118      0.747      0.724      0.748      0.426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      61/80      2.29G      1.194       1.22      1.106         30        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4ss.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.9it/s 0.1s\n",
      "                   all         20        118      0.778      0.747      0.761      0.437\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      62/80      2.29G      1.175      1.205      1.091         39        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.9it/s 0.1s\n",
      "                   all         20        118      0.798      0.701      0.764      0.442\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      63/80      2.29G      1.196      1.222      1.093         47        640: 100% ━━━━━━━━━━━━ 14/14 10.1it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.8it/s 0.1s\n",
      "                   all         20        118      0.795      0.711      0.749      0.434\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      64/80      2.29G      1.178      1.179      1.097         26        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.1it/s 0.1s\n",
      "                   all         20        118      0.813      0.689      0.761      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      65/80      2.29G      1.192      1.191      1.103         38        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.5it/s 0.1s\n",
      "                   all         20        118      0.813      0.726      0.764      0.442\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      66/80      2.29G      1.182      1.193      1.104         32        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.9it/s 0.1s\n",
      "                   all         20        118      0.845      0.713      0.767      0.438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      67/80      2.29G       1.19      1.189      1.092         43        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.5it/s 0.1s\n",
      "                   all         20        118      0.846      0.688      0.759      0.426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      68/80      2.29G      1.191      1.172      1.082         51        640: 100% ━━━━━━━━━━━━ 14/14 9.9it/s 1.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.6it/s 0.1s\n",
      "                   all         20        118      0.781      0.718      0.758      0.428\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      69/80      2.29G      1.157      1.144      1.078         31        640: 100% ━━━━━━━━━━━━ 14/14 10.1it/s 1.4s.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.6it/s 0.1s\n",
      "                   all         20        118      0.835      0.698      0.761      0.431\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      70/80      2.29G      1.151      1.152      1.085         37        640: 100% ━━━━━━━━━━━━ 14/14 10.0it/s 1.4s.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 9.0it/s 0.1s\n",
      "                   all         20        118      0.812      0.743      0.774      0.453\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      71/80      2.29G      1.152      1.175      1.097         17        640: 100% ━━━━━━━━━━━━ 14/14 8.2it/s 1.7s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.5it/s 0.1s\n",
      "                   all         20        118      0.818      0.741      0.766      0.444\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      72/80      2.29G      1.158      1.155      1.105         31        640: 100% ━━━━━━━━━━━━ 14/14 10.2it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.0it/s 0.1s\n",
      "                   all         20        118      0.796      0.735      0.775      0.445\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      73/80      2.29G      1.131      1.105      1.091         34        640: 100% ━━━━━━━━━━━━ 14/14 10.2it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.7it/s 0.1s\n",
      "                   all         20        118      0.801      0.748      0.772      0.441\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      74/80      2.29G      1.108      1.071      1.073         36        640: 100% ━━━━━━━━━━━━ 14/14 10.2it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.3it/s 0.1s\n",
      "                   all         20        118      0.803      0.738      0.761      0.436\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      75/80      2.29G      1.113      1.061      1.079         20        640: 100% ━━━━━━━━━━━━ 14/14 10.2it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.6it/s 0.1s\n",
      "                   all         20        118      0.761      0.757      0.762      0.438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      76/80      2.29G      1.107      1.071      1.079         26        640: 100% ━━━━━━━━━━━━ 14/14 10.2it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.7it/s 0.1s\n",
      "                   all         20        118      0.793       0.74       0.76      0.432\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      77/80      2.29G      1.083      1.026      1.062         35        640: 100% ━━━━━━━━━━━━ 14/14 10.2it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.0it/s 0.1s\n",
      "                   all         20        118      0.791      0.731      0.757      0.429\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      78/80      2.29G      1.078      1.057      1.052         34        640: 100% ━━━━━━━━━━━━ 14/14 10.2it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.1it/s 0.1s\n",
      "                   all         20        118      0.789      0.731       0.76      0.426\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      79/80      2.29G      1.108      1.044      1.075         30        640: 100% ━━━━━━━━━━━━ 14/14 10.2it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 11.4it/s 0.1s\n",
      "                   all         20        118      0.789      0.725      0.757      0.434\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      80/80      2.29G       1.07      1.031      1.046         23        640: 100% ━━━━━━━━━━━━ 14/14 10.2it/s 1.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.8it/s 0.1s\n",
      "                   all         20        118      0.786      0.724       0.76      0.441\n",
      "\n",
      "80 epochs completed in 0.042 hours.\n",
      "Optimizer stripped from runs\\detect\\train17\\weights\\last.pt, 6.3MB\n",
      "Optimizer stripped from runs\\detect\\train17\\weights\\best.pt, 6.3MB\n",
      "\n",
      "Validating runs\\detect\\train17\\weights\\best.pt...\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Model summary (fused): 72 layers, 3,007,793 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 10.3it/s 0.1s\n",
      "                   all         20        118      0.839      0.742       0.78      0.459\n",
      "                     0          7          9      0.868          1      0.918      0.535\n",
      "                     1          8          9      0.993          1      0.995      0.641\n",
      "                     2          7          9      0.719      0.333      0.638      0.427\n",
      "                     3         12         15      0.726        0.6      0.754      0.433\n",
      "                     4         12         19      0.844          1       0.99      0.531\n",
      "                     5          5          5      0.588        0.6      0.458      0.305\n",
      "                     6         10         12      0.825      0.833      0.788      0.409\n",
      "                     7          8         13          1       0.89      0.968      0.598\n",
      "                     8          7         11      0.834      0.909      0.981      0.605\n",
      "                     9          8         11      0.835          1      0.942      0.463\n",
      "                 noise          5          5          1          0      0.145     0.0976\n",
      "Speed: 0.2ms preprocess, 1.3ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train17\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x00000240B442D360>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,         0.9,         0.9,           0],\n",
       "       [          1,           1,           1, ...,           1,           1,           0],\n",
       "       [          1,           1,           1, ...,     0.28125,     0.28125,           0],\n",
       "       ...,\n",
       "       [          1,           1,           1, ...,     0.84615,     0.84615,           0],\n",
       "       [          1,           1,           1, ...,     0.91667,     0.91667,           0],\n",
       "       [        0.5,         0.5,         0.5, ...,    0.044643,    0.044643,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.11538,     0.11538,     0.21132, ...,           0,           0,           0],\n",
       "       [      0.125,       0.125,     0.23075, ...,           0,           0,           0],\n",
       "       [    0.06383,     0.06383,     0.15832, ...,           0,           0,           0],\n",
       "       ...,\n",
       "       [   0.068323,    0.068323,     0.11685, ...,           0,           0,           0],\n",
       "       [   0.094421,    0.094421,     0.15931, ...,           0,           0,           0],\n",
       "       [   0.067568,    0.067568,    0.070569, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[   0.061224,    0.061224,     0.11814, ...,           1,           1,           1],\n",
       "       [   0.066667,    0.066667,     0.13042, ...,           1,           1,           1],\n",
       "       [   0.032967,    0.032967,    0.085966, ...,           1,           1,           1],\n",
       "       ...,\n",
       "       [    0.03537,     0.03537,    0.062048, ...,           1,           1,           1],\n",
       "       [    0.04955,     0.04955,    0.086551, ...,           1,           1,           1],\n",
       "       [   0.034965,    0.034965,    0.037489, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       ...,\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,           1, ...,           0,           0,           0],\n",
       "       [          1,           1,         0.6, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: np.float64(0.4907494433363963)\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.53462,     0.64142,     0.42676,     0.43289,     0.53101,     0.30528,     0.40948,     0.59813,     0.60469,     0.46306,    0.097647])\n",
       "names: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: 'noise'}\n",
       "nt_per_class: array([ 9,  9,  9, 15, 19,  5, 12, 13, 11, 11,  5])\n",
       "nt_per_image: array([ 7,  8,  7, 12, 12,  5, 10,  8,  7,  8,  5])\n",
       "results_dict: {'metrics/precision(B)': 0.8392208841296611, 'metrics/recall(B)': 0.7423228154157897, 'metrics/mAP50(B)': 0.7797754305959063, 'metrics/mAP50-95(B)': 0.4586354447520063, 'fitness': 0.4907494433363963}\n",
       "save_dir: WindowsPath('runs/detect/train17')\n",
       "speed: {'preprocess': 0.22072499996284023, 'inference': 1.3459900001180358, 'loss': 0.00026999914553016424, 'postprocess': 0.6431700006942265}\n",
       "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# YOLOv8 기본 모델 불러오기\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# 데이터셋 경로\n",
    "data_path = \"C:/githome/Object_detection/Num.yolov8/data.yaml\"\n",
    "\n",
    "# 간단 학습 (5 epoch)\n",
    "model.train(\n",
    "    data=data_path,\n",
    "    epochs=80,\n",
    "    imgsz=640\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84db0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Model summary (fused): 72 layers, 3,007,793 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 124.934.2 MB/s, size: 10.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\githome\\Object_detection\\Num.yolov8\\valid\\labels.cache... 20 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 20/20 19916.0it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 0.86it/s 2.3s7.3s\n",
      "                   all         20        118      0.844      0.752      0.783      0.462\n",
      "                     0          7          9      0.867          1      0.918      0.542\n",
      "                     1          8          9      0.992          1      0.995      0.631\n",
      "                     2          7          9      0.703      0.333      0.654      0.438\n",
      "                     3         12         15       0.72        0.6       0.76      0.435\n",
      "                     4         12         19       0.84          1       0.99      0.532\n",
      "                     5          5          5      0.582        0.6      0.458      0.299\n",
      "                     6         10         12      0.836      0.852      0.797       0.42\n",
      "                     7          8         13          1      0.892      0.966      0.611\n",
      "                     8          7         11      0.916      0.997      0.988      0.609\n",
      "                     9          8         11      0.829          1      0.942      0.462\n",
      "                 noise          5          5          1          0      0.145     0.0976\n",
      "Speed: 4.0ms preprocess, 21.4ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_model = YOLO(\"runs/detect/train17/weights/best.pt\")\n",
    "\n",
    "data_path = \"C:/githome/Object_detection/Num.yolov8/data.yaml\"\n",
    "metrics = best_model.val(data=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bf49bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTuner: \u001b[0mInitialized Tuner instance with 'tune_dir=runs\\detect\\tune3'\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m Learn about tuning at https://docs.ultralytics.com/guides/hyperparameter-tuning\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 1/25 with hyperparameters: {'lr0': 0.01, 'lrf': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'warmup_epochs': 3.0, 'warmup_momentum': 0.8, 'box': 7.5, 'cls': 0.5, 'dfl': 1.5, 'hsv_h': 0.015, 'hsv_s': 0.7, 'hsv_v': 0.4, 'degrees': 0.0, 'translate': 0.1, 'scale': 0.5, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.5, 'bgr': 0.0, 'mosaic': 1.0, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m1/25 iterations complete  (82.82s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.23883 observed at iteration 1\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.4318, 'metrics/recall(B)': 0.3485, 'metrics/mAP50(B)': 0.42527, 'metrics/mAP50-95(B)': 0.21812, 'val/box_loss': 1.37823, 'val/cls_loss': 2.28008, 'val/dfl_loss': 1.12764, 'fitness': 0.23883}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train18\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.01\n",
      "lrf: 0.01\n",
      "momentum: 0.937\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 3.0\n",
      "warmup_momentum: 0.8\n",
      "box: 7.5\n",
      "cls: 0.5\n",
      "dfl: 1.5\n",
      "hsv_h: 0.015\n",
      "hsv_s: 0.7\n",
      "hsv_v: 0.4\n",
      "degrees: 0.0\n",
      "translate: 0.1\n",
      "scale: 0.5\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.5\n",
      "bgr: 0.0\n",
      "mosaic: 1.0\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 2/25 with hyperparameters: {'lr0': 0.00952, 'lrf': 0.01031, 'momentum': 0.93476, 'weight_decay': 0.0005, 'warmup_epochs': 2.82095, 'warmup_momentum': 0.8, 'box': 7.04206, 'cls': 0.5, 'dfl': 1.46735, 'hsv_h': 0.01484, 'hsv_s': 0.7, 'hsv_v': 0.40123, 'degrees': 0.0, 'translate': 0.10401, 'scale': 0.53422, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.49988, 'bgr': 0.0, 'mosaic': 0.99919, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m2/25 iterations complete  (163.85s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.24417 observed at iteration 2\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.4746, 'metrics/recall(B)': 0.38094, 'metrics/mAP50(B)': 0.41121, 'metrics/mAP50-95(B)': 0.22562, 'val/box_loss': 1.35017, 'val/cls_loss': 2.20556, 'val/dfl_loss': 1.12758, 'fitness': 0.24417}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train19\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00952\n",
      "lrf: 0.01031\n",
      "momentum: 0.93476\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 2.82095\n",
      "warmup_momentum: 0.8\n",
      "box: 7.04206\n",
      "cls: 0.5\n",
      "dfl: 1.46735\n",
      "hsv_h: 0.01484\n",
      "hsv_s: 0.7\n",
      "hsv_v: 0.40123\n",
      "degrees: 0.0\n",
      "translate: 0.10401\n",
      "scale: 0.53422\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.49988\n",
      "bgr: 0.0\n",
      "mosaic: 0.99919\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 3/25 with hyperparameters: {'lr0': 0.00953, 'lrf': 0.01027, 'momentum': 0.93476, 'weight_decay': 0.0005, 'warmup_epochs': 2.8236, 'warmup_momentum': 0.79929, 'box': 7.04206, 'cls': 0.49782, 'dfl': 1.47059, 'hsv_h': 0.01484, 'hsv_s': 0.7014, 'hsv_v': 0.39986, 'degrees': 0.0, 'translate': 0.10415, 'scale': 0.53067, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.50064, 'bgr': 0.0, 'mosaic': 0.9955, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m3/25 iterations complete  (244.50s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.24647 observed at iteration 3\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.41365, 'metrics/recall(B)': 0.32357, 'metrics/mAP50(B)': 0.40334, 'metrics/mAP50-95(B)': 0.22904, 'val/box_loss': 1.30518, 'val/cls_loss': 2.23253, 'val/dfl_loss': 1.1199, 'fitness': 0.24647}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train20\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00953\n",
      "lrf: 0.01027\n",
      "momentum: 0.93476\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 2.8236\n",
      "warmup_momentum: 0.79929\n",
      "box: 7.04206\n",
      "cls: 0.49782\n",
      "dfl: 1.47059\n",
      "hsv_h: 0.01484\n",
      "hsv_s: 0.7014\n",
      "hsv_v: 0.39986\n",
      "degrees: 0.0\n",
      "translate: 0.10415\n",
      "scale: 0.53067\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.50064\n",
      "bgr: 0.0\n",
      "mosaic: 0.9955\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 4/25 with hyperparameters: {'lr0': 0.01009, 'lrf': 0.01172, 'momentum': 0.93476, 'weight_decay': 0.0005, 'warmup_epochs': 3.29473, 'warmup_momentum': 0.92827, 'box': 7.23039, 'cls': 0.60701, 'dfl': 1.46735, 'hsv_h': 0.01618, 'hsv_s': 0.72122, 'hsv_v': 0.40123, 'degrees': 0.0, 'translate': 0.12618, 'scale': 0.59775, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.49988, 'bgr': 0.0, 'mosaic': 0.75373, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m4/25 iterations complete  (323.73s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.24647 observed at iteration 3\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.41365, 'metrics/recall(B)': 0.32357, 'metrics/mAP50(B)': 0.40334, 'metrics/mAP50-95(B)': 0.22904, 'val/box_loss': 1.30518, 'val/cls_loss': 2.23253, 'val/dfl_loss': 1.1199, 'fitness': 0.24647}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train20\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00953\n",
      "lrf: 0.01027\n",
      "momentum: 0.93476\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 2.8236\n",
      "warmup_momentum: 0.79929\n",
      "box: 7.04206\n",
      "cls: 0.49782\n",
      "dfl: 1.47059\n",
      "hsv_h: 0.01484\n",
      "hsv_s: 0.7014\n",
      "hsv_v: 0.39986\n",
      "degrees: 0.0\n",
      "translate: 0.10415\n",
      "scale: 0.53067\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.50064\n",
      "bgr: 0.0\n",
      "mosaic: 0.9955\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 5/25 with hyperparameters: {'lr0': 0.00822, 'lrf': 0.01031, 'momentum': 0.98, 'weight_decay': 0.0005, 'warmup_epochs': 2.25868, 'warmup_momentum': 0.7609, 'box': 7.04605, 'cls': 0.45575, 'dfl': 1.55338, 'hsv_h': 0.01484, 'hsv_s': 0.64829, 'hsv_v': 0.40123, 'degrees': 0.0, 'translate': 0.10367, 'scale': 0.53422, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.49988, 'bgr': 0.0, 'mosaic': 0.99919, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m5/25 iterations complete  (403.39s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.24647 observed at iteration 3\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.41365, 'metrics/recall(B)': 0.32357, 'metrics/mAP50(B)': 0.40334, 'metrics/mAP50-95(B)': 0.22904, 'val/box_loss': 1.30518, 'val/cls_loss': 2.23253, 'val/dfl_loss': 1.1199, 'fitness': 0.24647}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train20\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00953\n",
      "lrf: 0.01027\n",
      "momentum: 0.93476\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 2.8236\n",
      "warmup_momentum: 0.79929\n",
      "box: 7.04206\n",
      "cls: 0.49782\n",
      "dfl: 1.47059\n",
      "hsv_h: 0.01484\n",
      "hsv_s: 0.7014\n",
      "hsv_v: 0.39986\n",
      "degrees: 0.0\n",
      "translate: 0.10415\n",
      "scale: 0.53067\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.50064\n",
      "bgr: 0.0\n",
      "mosaic: 0.9955\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 6/25 with hyperparameters: {'lr0': 0.00781, 'lrf': 0.01, 'momentum': 0.95005, 'weight_decay': 0.00054, 'warmup_epochs': 2.99703, 'warmup_momentum': 0.66775, 'box': 7.87074, 'cls': 0.58812, 'dfl': 1.61763, 'hsv_h': 0.01628, 'hsv_s': 0.7084, 'hsv_v': 0.43039, 'degrees': 0.0, 'translate': 0.09113, 'scale': 0.3944, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.4024, 'bgr': 0.0, 'mosaic': 0.94376, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m6/25 iterations complete  (482.37s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.24647 observed at iteration 3\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.41365, 'metrics/recall(B)': 0.32357, 'metrics/mAP50(B)': 0.40334, 'metrics/mAP50-95(B)': 0.22904, 'val/box_loss': 1.30518, 'val/cls_loss': 2.23253, 'val/dfl_loss': 1.1199, 'fitness': 0.24647}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train20\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00953\n",
      "lrf: 0.01027\n",
      "momentum: 0.93476\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 2.8236\n",
      "warmup_momentum: 0.79929\n",
      "box: 7.04206\n",
      "cls: 0.49782\n",
      "dfl: 1.47059\n",
      "hsv_h: 0.01484\n",
      "hsv_s: 0.7014\n",
      "hsv_v: 0.39986\n",
      "degrees: 0.0\n",
      "translate: 0.10415\n",
      "scale: 0.53067\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.50064\n",
      "bgr: 0.0\n",
      "mosaic: 0.9955\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 7/25 with hyperparameters: {'lr0': 0.00856, 'lrf': 0.00917, 'momentum': 0.90106, 'weight_decay': 0.0005, 'warmup_epochs': 2.79989, 'warmup_momentum': 0.81567, 'box': 6.98487, 'cls': 0.57198, 'dfl': 2.55389, 'hsv_h': 0.01484, 'hsv_s': 0.84243, 'hsv_v': 0.42694, 'degrees': 0.0, 'translate': 0.10415, 'scale': 0.68554, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.60661, 'bgr': 0.0, 'mosaic': 0.71654, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m7/25 iterations complete  (562.92s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.24647 observed at iteration 3\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.41365, 'metrics/recall(B)': 0.32357, 'metrics/mAP50(B)': 0.40334, 'metrics/mAP50-95(B)': 0.22904, 'val/box_loss': 1.30518, 'val/cls_loss': 2.23253, 'val/dfl_loss': 1.1199, 'fitness': 0.24647}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train20\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00953\n",
      "lrf: 0.01027\n",
      "momentum: 0.93476\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 2.8236\n",
      "warmup_momentum: 0.79929\n",
      "box: 7.04206\n",
      "cls: 0.49782\n",
      "dfl: 1.47059\n",
      "hsv_h: 0.01484\n",
      "hsv_s: 0.7014\n",
      "hsv_v: 0.39986\n",
      "degrees: 0.0\n",
      "translate: 0.10415\n",
      "scale: 0.53067\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.50064\n",
      "bgr: 0.0\n",
      "mosaic: 0.9955\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 8/25 with hyperparameters: {'lr0': 0.00809, 'lrf': 0.01233, 'momentum': 0.98, 'weight_decay': 0.00057, 'warmup_epochs': 2.70675, 'warmup_momentum': 0.82505, 'box': 6.84097, 'cls': 0.32362, 'dfl': 1.26224, 'hsv_h': 0.01484, 'hsv_s': 0.73695, 'hsv_v': 0.32982, 'degrees': 0.0, 'translate': 0.10367, 'scale': 0.53422, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.49988, 'bgr': 0.0, 'mosaic': 0.96028, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m8/25 iterations complete  (643.52s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.24647 observed at iteration 3\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.41365, 'metrics/recall(B)': 0.32357, 'metrics/mAP50(B)': 0.40334, 'metrics/mAP50-95(B)': 0.22904, 'val/box_loss': 1.30518, 'val/cls_loss': 2.23253, 'val/dfl_loss': 1.1199, 'fitness': 0.24647}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train20\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00953\n",
      "lrf: 0.01027\n",
      "momentum: 0.93476\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 2.8236\n",
      "warmup_momentum: 0.79929\n",
      "box: 7.04206\n",
      "cls: 0.49782\n",
      "dfl: 1.47059\n",
      "hsv_h: 0.01484\n",
      "hsv_s: 0.7014\n",
      "hsv_v: 0.39986\n",
      "degrees: 0.0\n",
      "translate: 0.10415\n",
      "scale: 0.53067\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.50064\n",
      "bgr: 0.0\n",
      "mosaic: 0.9955\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 9/25 with hyperparameters: {'lr0': 0.01002, 'lrf': 0.01005, 'momentum': 0.95057, 'weight_decay': 0.00052, 'warmup_epochs': 2.94212, 'warmup_momentum': 0.82114, 'box': 6.99749, 'cls': 0.49442, 'dfl': 1.52628, 'hsv_h': 0.01484, 'hsv_s': 0.72785, 'hsv_v': 0.42216, 'degrees': 0.0, 'translate': 0.10401, 'scale': 0.50128, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.49988, 'bgr': 0.0, 'mosaic': 1.0, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m9/25 iterations complete  (722.93s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.24647 observed at iteration 3\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.41365, 'metrics/recall(B)': 0.32357, 'metrics/mAP50(B)': 0.40334, 'metrics/mAP50-95(B)': 0.22904, 'val/box_loss': 1.30518, 'val/cls_loss': 2.23253, 'val/dfl_loss': 1.1199, 'fitness': 0.24647}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train20\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00953\n",
      "lrf: 0.01027\n",
      "momentum: 0.93476\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 2.8236\n",
      "warmup_momentum: 0.79929\n",
      "box: 7.04206\n",
      "cls: 0.49782\n",
      "dfl: 1.47059\n",
      "hsv_h: 0.01484\n",
      "hsv_s: 0.7014\n",
      "hsv_v: 0.39986\n",
      "degrees: 0.0\n",
      "translate: 0.10415\n",
      "scale: 0.53067\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.50064\n",
      "bgr: 0.0\n",
      "mosaic: 0.9955\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 10/25 with hyperparameters: {'lr0': 0.00964, 'lrf': 0.01092, 'momentum': 0.91097, 'weight_decay': 0.00053, 'warmup_epochs': 2.8236, 'warmup_momentum': 0.82302, 'box': 6.97398, 'cls': 0.48615, 'dfl': 1.54719, 'hsv_h': 0.01439, 'hsv_s': 0.70246, 'hsv_v': 0.39403, 'degrees': 0.0, 'translate': 0.10549, 'scale': 0.50761, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.54334, 'bgr': 0.0, 'mosaic': 1.0, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m10/25 iterations complete  (802.92s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.24647 observed at iteration 3\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.41365, 'metrics/recall(B)': 0.32357, 'metrics/mAP50(B)': 0.40334, 'metrics/mAP50-95(B)': 0.22904, 'val/box_loss': 1.30518, 'val/cls_loss': 2.23253, 'val/dfl_loss': 1.1199, 'fitness': 0.24647}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train20\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00953\n",
      "lrf: 0.01027\n",
      "momentum: 0.93476\n",
      "weight_decay: 0.0005\n",
      "warmup_epochs: 2.8236\n",
      "warmup_momentum: 0.79929\n",
      "box: 7.04206\n",
      "cls: 0.49782\n",
      "dfl: 1.47059\n",
      "hsv_h: 0.01484\n",
      "hsv_s: 0.7014\n",
      "hsv_v: 0.39986\n",
      "degrees: 0.0\n",
      "translate: 0.10415\n",
      "scale: 0.53067\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.50064\n",
      "bgr: 0.0\n",
      "mosaic: 0.9955\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 11/25 with hyperparameters: {'lr0': 0.00784, 'lrf': 0.00818, 'momentum': 0.95825, 'weight_decay': 0.00055, 'warmup_epochs': 2.99703, 'warmup_momentum': 0.69302, 'box': 8.01057, 'cls': 0.62774, 'dfl': 1.7449, 'hsv_h': 0.02015, 'hsv_s': 0.43594, 'hsv_v': 0.41619, 'degrees': 0.0, 'translate': 0.11219, 'scale': 0.42777, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.41987, 'bgr': 0.0, 'mosaic': 0.97019, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m11/25 iterations complete  (881.94s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.27496 observed at iteration 11\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.5061, 'metrics/recall(B)': 0.36502, 'metrics/mAP50(B)': 0.47918, 'metrics/mAP50-95(B)': 0.25227, 'val/box_loss': 1.52364, 'val/cls_loss': 2.84785, 'val/dfl_loss': 1.34191, 'fitness': 0.27496}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train28\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00784\n",
      "lrf: 0.00818\n",
      "momentum: 0.95825\n",
      "weight_decay: 0.00055\n",
      "warmup_epochs: 2.99703\n",
      "warmup_momentum: 0.69302\n",
      "box: 8.01057\n",
      "cls: 0.62774\n",
      "dfl: 1.7449\n",
      "hsv_h: 0.02015\n",
      "hsv_s: 0.43594\n",
      "hsv_v: 0.41619\n",
      "degrees: 0.0\n",
      "translate: 0.11219\n",
      "scale: 0.42777\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.41987\n",
      "bgr: 0.0\n",
      "mosaic: 0.97019\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 12/25 with hyperparameters: {'lr0': 0.00749, 'lrf': 0.00822, 'momentum': 0.94717, 'weight_decay': 0.00059, 'warmup_epochs': 2.87462, 'warmup_momentum': 0.7072, 'box': 7.77401, 'cls': 0.60987, 'dfl': 1.70757, 'hsv_h': 0.02057, 'hsv_s': 0.43594, 'hsv_v': 0.40721, 'degrees': 0.0, 'translate': 0.11375, 'scale': 0.42777, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.47871, 'bgr': 0.0, 'mosaic': 1.0, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m12/25 iterations complete  (962.29s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.27496 observed at iteration 11\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.5061, 'metrics/recall(B)': 0.36502, 'metrics/mAP50(B)': 0.47918, 'metrics/mAP50-95(B)': 0.25227, 'val/box_loss': 1.52364, 'val/cls_loss': 2.84785, 'val/dfl_loss': 1.34191, 'fitness': 0.27496}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train28\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00784\n",
      "lrf: 0.00818\n",
      "momentum: 0.95825\n",
      "weight_decay: 0.00055\n",
      "warmup_epochs: 2.99703\n",
      "warmup_momentum: 0.69302\n",
      "box: 8.01057\n",
      "cls: 0.62774\n",
      "dfl: 1.7449\n",
      "hsv_h: 0.02015\n",
      "hsv_s: 0.43594\n",
      "hsv_v: 0.41619\n",
      "degrees: 0.0\n",
      "translate: 0.11219\n",
      "scale: 0.42777\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.41987\n",
      "bgr: 0.0\n",
      "mosaic: 0.97019\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 13/25 with hyperparameters: {'lr0': 0.00813, 'lrf': 0.00678, 'momentum': 0.93768, 'weight_decay': 0.00067, 'warmup_epochs': 1.76759, 'warmup_momentum': 0.68208, 'box': 7.55679, 'cls': 0.48978, 'dfl': 1.737, 'hsv_h': 0.02057, 'hsv_s': 0.51016, 'hsv_v': 0.40128, 'degrees': 0.0, 'translate': 0.17064, 'scale': 0.41409, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.43735, 'bgr': 0.0, 'mosaic': 0.82657, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m13/25 iterations complete  (1043.06s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 14/25 with hyperparameters: {'lr0': 0.00777, 'lrf': 0.00872, 'momentum': 0.95615, 'weight_decay': 0.00059, 'warmup_epochs': 2.99703, 'warmup_momentum': 0.61787, 'box': 8.29416, 'cls': 0.65312, 'dfl': 1.66063, 'hsv_h': 0.02037, 'hsv_s': 0.42963, 'hsv_v': 0.41402, 'degrees': 0.0, 'translate': 0.10874, 'scale': 0.42809, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.41987, 'bgr': 0.0, 'mosaic': 0.97019, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m14/25 iterations complete  (1124.76s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 15/25 with hyperparameters: {'lr0': 0.00813, 'lrf': 0.00678, 'momentum': 0.93285, 'weight_decay': 0.00066, 'warmup_epochs': 1.47424, 'warmup_momentum': 0.58111, 'box': 8.47399, 'cls': 0.53971, 'dfl': 1.75695, 'hsv_h': 0.02028, 'hsv_s': 0.56434, 'hsv_v': 0.33288, 'degrees': 0.0, 'translate': 0.15784, 'scale': 0.35691, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.37578, 'bgr': 0.0, 'mosaic': 0.82657, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m15/25 iterations complete  (1203.47s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 16/25 with hyperparameters: {'lr0': 0.00854, 'lrf': 0.00944, 'momentum': 0.86393, 'weight_decay': 0.00052, 'warmup_epochs': 2.63433, 'warmup_momentum': 0.69302, 'box': 6.94518, 'cls': 0.54912, 'dfl': 1.61272, 'hsv_h': 0.02442, 'hsv_s': 0.38237, 'hsv_v': 0.40219, 'degrees': 0.0, 'translate': 0.11978, 'scale': 0.44322, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.53357, 'bgr': 0.0, 'mosaic': 0.97019, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m16/25 iterations complete  (1283.64s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 17/25 with hyperparameters: {'lr0': 0.00749, 'lrf': 0.00845, 'momentum': 0.9672, 'weight_decay': 0.00067, 'warmup_epochs': 2.87462, 'warmup_momentum': 0.7072, 'box': 7.77401, 'cls': 0.61323, 'dfl': 1.70919, 'hsv_h': 0.02027, 'hsv_s': 0.43818, 'hsv_v': 0.38086, 'degrees': 0.0, 'translate': 0.10522, 'scale': 0.42242, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.49154, 'bgr': 0.0, 'mosaic': 1.0, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m17/25 iterations complete  (1362.54s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 18/25 with hyperparameters: {'lr0': 0.00813, 'lrf': 0.00695, 'momentum': 0.94217, 'weight_decay': 0.00067, 'warmup_epochs': 1.68842, 'warmup_momentum': 0.66654, 'box': 7.40139, 'cls': 0.48978, 'dfl': 1.73137, 'hsv_h': 0.02019, 'hsv_s': 0.5074, 'hsv_v': 0.41277, 'degrees': 0.0, 'translate': 0.17369, 'scale': 0.41496, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.44852, 'bgr': 0.0, 'mosaic': 0.82424, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m18/25 iterations complete  (1442.13s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 19/25 with hyperparameters: {'lr0': 0.00813, 'lrf': 0.00825, 'momentum': 0.92877, 'weight_decay': 0.00067, 'warmup_epochs': 1.78839, 'warmup_momentum': 0.54623, 'box': 7.55679, 'cls': 0.49206, 'dfl': 1.90385, 'hsv_h': 0.02189, 'hsv_s': 0.50543, 'hsv_v': 0.39431, 'degrees': 0.0, 'translate': 0.15872, 'scale': 0.41409, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.40888, 'bgr': 0.0, 'mosaic': 0.79885, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m19/25 iterations complete  (1521.82s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 20/25 with hyperparameters: {'lr0': 0.0083, 'lrf': 0.00964, 'momentum': 0.97631, 'weight_decay': 0.00067, 'warmup_epochs': 2.44689, 'warmup_momentum': 0.71739, 'box': 7.06957, 'cls': 0.61422, 'dfl': 1.70757, 'hsv_h': 0.0227, 'hsv_s': 0.46999, 'hsv_v': 0.40309, 'degrees': 0.0, 'translate': 0.12145, 'scale': 0.40635, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.47871, 'bgr': 0.0, 'mosaic': 1.0, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m20/25 iterations complete  (1599.76s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 21/25 with hyperparameters: {'lr0': 0.00846, 'lrf': 0.00617, 'momentum': 0.90247, 'weight_decay': 0.00065, 'warmup_epochs': 1.74235, 'warmup_momentum': 0.68208, 'box': 7.55679, 'cls': 0.44675, 'dfl': 1.737, 'hsv_h': 0.02259, 'hsv_s': 0.56433, 'hsv_v': 0.3877, 'degrees': 0.0, 'translate': 0.2038, 'scale': 0.36879, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.49616, 'bgr': 0.0, 'mosaic': 0.85165, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m21/25 iterations complete  (1677.98s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 22/25 with hyperparameters: {'lr0': 0.00892, 'lrf': 0.00818, 'momentum': 0.94303, 'weight_decay': 0.00055, 'warmup_epochs': 3.3297, 'warmup_momentum': 0.66035, 'box': 8.21604, 'cls': 0.57813, 'dfl': 1.35843, 'hsv_h': 0.02547, 'hsv_s': 0.33089, 'hsv_v': 0.51963, 'degrees': 0.0, 'translate': 0.13474, 'scale': 0.46396, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.41987, 'bgr': 0.0, 'mosaic': 0.97019, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m22/25 iterations complete  (1756.39s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 23/25 with hyperparameters: {'lr0': 0.0084, 'lrf': 0.00733, 'momentum': 0.93768, 'weight_decay': 0.0007, 'warmup_epochs': 1.84472, 'warmup_momentum': 0.64856, 'box': 7.15674, 'cls': 0.48875, 'dfl': 1.79908, 'hsv_h': 0.02057, 'hsv_s': 0.45206, 'hsv_v': 0.37288, 'degrees': 0.0, 'translate': 0.18935, 'scale': 0.39872, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.43044, 'bgr': 0.0, 'mosaic': 0.84482, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m23/25 iterations complete  (1836.61s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 24/25 with hyperparameters: {'lr0': 0.00846, 'lrf': 0.00842, 'momentum': 0.94406, 'weight_decay': 0.00068, 'warmup_epochs': 1.78839, 'warmup_momentum': 0.54623, 'box': 7.55679, 'cls': 0.47962, 'dfl': 1.86705, 'hsv_h': 0.02143, 'hsv_s': 0.4805, 'hsv_v': 0.39387, 'degrees': 0.0, 'translate': 0.15872, 'scale': 0.41117, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.42087, 'bgr': 0.0, 'mosaic': 0.82523, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m24/25 iterations complete  (1918.57s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mStarting iteration 25/25 with hyperparameters: {'lr0': 0.00699, 'lrf': 0.00644, 'momentum': 0.96362, 'weight_decay': 0.00039, 'warmup_epochs': 3.24118, 'warmup_momentum': 0.49967, 'box': 10.57033, 'cls': 0.61093, 'dfl': 1.61678, 'hsv_h': 0.01853, 'hsv_s': 0.43298, 'hsv_v': 0.38036, 'degrees': 0.0, 'translate': 0.09172, 'scale': 0.42777, 'shear': 0.0, 'perspective': 0.0, 'flipud': 0.0, 'fliplr': 0.42672, 'bgr': 0.0, 'mosaic': 1.0, 'mixup': 0.0, 'cutmix': 0.0, 'copy_paste': 0.0}\n",
      "Saved runs\\detect\\tune3\\tune_scatter_plots.png\n",
      "Saved runs\\detect\\tune3\\tune_fitness.png\n",
      "\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0m25/25 iterations complete  (1999.53s)\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mResults saved to \u001b[1mruns\\detect\\tune3\u001b[0m\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness=0.2972 observed at iteration 13\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness metrics are {'metrics/precision(B)': 0.56964, 'metrics/recall(B)': 0.38646, 'metrics/mAP50(B)': 0.48628, 'metrics/mAP50-95(B)': 0.27619, 'val/box_loss': 1.42492, 'val/cls_loss': 2.17764, 'val/dfl_loss': 1.32553, 'fitness': 0.2972}\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness model is runs\\detect\\train30\n",
      "\u001b[34m\u001b[1mTuner: \u001b[0mBest fitness hyperparameters are printed below.\n",
      "\n",
      "Printing '\u001b[1m\u001b[30mruns\\detect\\tune3\\best_hyperparameters.yaml\u001b[0m'\n",
      "\n",
      "lr0: 0.00813\n",
      "lrf: 0.00678\n",
      "momentum: 0.93768\n",
      "weight_decay: 0.00067\n",
      "warmup_epochs: 1.76759\n",
      "warmup_momentum: 0.68208\n",
      "box: 7.55679\n",
      "cls: 0.48978\n",
      "dfl: 1.737\n",
      "hsv_h: 0.02057\n",
      "hsv_s: 0.51016\n",
      "hsv_v: 0.40128\n",
      "degrees: 0.0\n",
      "translate: 0.17064\n",
      "scale: 0.41409\n",
      "shear: 0.0\n",
      "perspective: 0.0\n",
      "flipud: 0.0\n",
      "fliplr: 0.43735\n",
      "bgr: 0.0\n",
      "mosaic: 0.82657\n",
      "mixup: 0.0\n",
      "cutmix: 0.0\n",
      "copy_paste: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# YOLOv8 모델 로드\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "# 하이퍼파라미터 탐색 수행\n",
    "model.tune(\n",
    "data=\"./Num.yolov8/data.yaml\",\n",
    "epochs=10,\n",
    "imgsz=640,\n",
    "iterations=25, # 탐색 횟수 (예: 25번의 실험)\n",
    "val=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50ee7f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.192 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.55679, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.48978, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./datasets/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.737, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.43735, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.02057, hsv_s=0.51016, hsv_v=0.40128, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.00813, lrf=0.00678, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.93768, mosaic=0.82657, multi_scale=False, name=train43, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train43, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.41409, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.17064, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=1.76759, warmup_momentum=0.68208, weight_decay=0.00067, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 8.02.3 MB/s, size: 41.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\githome\\Object_detection\\datasets\\train\\labels.cache... 6455 images, 2516 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 6455/6455  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 4.82.3 MB/s, size: 32.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\githome\\Object_detection\\datasets\\valid\\labels.cache... 576 images, 238 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 576/576 575767.2it/s 0.0s\n",
      "Plotting labels to runs\\detect\\train43\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.00813' and 'momentum=0.93768' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.00067), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train43\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/30      3.95G      1.323      2.988      1.716         32        640: 100% ━━━━━━━━━━━━ 202/202 4.7it/s 42.9s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.0it/s 2.3s0.3s\n",
      "                   all        576        400      0.615       0.25      0.318      0.182\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/30      3.86G      1.395      2.095      1.742         18        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.5s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.1it/s 2.2s0.3s\n",
      "                   all        576        400      0.569      0.413      0.476      0.251\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/30      3.86G      1.369      1.781      1.718         27        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.5s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.3it/s 2.1s0.2s\n",
      "                   all        576        400      0.584       0.54      0.555      0.345\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/30      3.86G      1.293      1.561      1.647         25        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.3it/s 2.1s0.2s\n",
      "                   all        576        400      0.697      0.689      0.748      0.493\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/30      3.86G      1.238       1.42      1.604         31        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.4s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.5it/s 2.0s0.2s\n",
      "                   all        576        400      0.731      0.688      0.765      0.527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/30      3.86G      1.185      1.291       1.56         17        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.4s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.3it/s 2.1s0.2s\n",
      "                   all        576        400      0.782      0.734      0.788      0.549\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/30      3.86G      1.116      1.172      1.507         23        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 40.9s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.0s0.2s\n",
      "                   all        576        400       0.88      0.826      0.882       0.63\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/30      3.86G      1.106      1.106      1.492         26        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.3it/s 2.1s0.2s\n",
      "                   all        576        400      0.797      0.855       0.86      0.618\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/30      3.86G      1.066      1.069       1.47         22        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.4s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.3it/s 2.1s0.2s\n",
      "                   all        576        400        0.9      0.815        0.9      0.655\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/30      3.86G       1.04     0.9969       1.44         22        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.3it/s 2.1s0.2s\n",
      "                   all        576        400      0.858      0.868      0.906      0.654\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/30      3.86G      1.008     0.9501      1.412         26        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 40.9s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.1s0.2s\n",
      "                   all        576        400      0.878      0.851      0.913      0.661\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/30      3.86G      1.002     0.9215      1.412         33        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.4s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.2it/s 2.1s0.2s\n",
      "                   all        576        400      0.877       0.87      0.918      0.679\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/30      3.86G     0.9834     0.8949      1.402         20        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.1s0.2s\n",
      "                   all        576        400      0.912      0.828      0.915      0.688\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/30      3.86G     0.9508     0.8815      1.379         32        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.2s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.0s0.2s\n",
      "                   all        576        400      0.919      0.847      0.908      0.671\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/30      3.86G      0.932     0.8091      1.361         33        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.2s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.5it/s 2.0s0.2s\n",
      "                   all        576        400      0.924      0.848      0.936      0.721\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/30      3.86G     0.9155     0.8026      1.354         23        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.3it/s 2.1s0.2s\n",
      "                   all        576        400      0.912      0.905      0.938      0.725\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/30      3.86G     0.8794     0.7721      1.328         28        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.5it/s 2.0s0.2s\n",
      "                   all        576        400      0.912      0.898      0.938      0.724\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/30      3.86G     0.8756     0.7376      1.319         30        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.4s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.0s0.2s\n",
      "                   all        576        400      0.882      0.925      0.935       0.72\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/30      3.86G     0.8589     0.7253      1.314         27        640: 100% ━━━━━━━━━━━━ 202/202 5.0it/s 40.5s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.0s0.2s\n",
      "                   all        576        400       0.93       0.91      0.936      0.729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/30      3.86G     0.8382     0.6966      1.296         30        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.0s0.2s\n",
      "                   all        576        400      0.902      0.915      0.942      0.727\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/30      3.86G     0.7211      0.463      1.207         17        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 40.9s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.5it/s 2.0s0.2s\n",
      "                   all        576        400      0.925      0.901      0.932      0.716\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/30      3.86G     0.6925     0.4299      1.192         16        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.0s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.3it/s 2.1s0.2s\n",
      "                   all        576        400      0.937      0.904      0.949      0.747\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/30      3.86G     0.6685     0.4121      1.178         15        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.1s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.1s0.2s\n",
      "                   all        576        400      0.939      0.905       0.95       0.75\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/30      3.86G     0.6478      0.397      1.166         15        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.0s0.2s\n",
      "                   all        576        400      0.937      0.918      0.947      0.755\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/30      3.86G     0.6252     0.3798      1.148         14        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.2s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.0s0.2s\n",
      "                   all        576        400      0.928      0.921       0.95      0.757\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/30      3.86G     0.6152     0.3639      1.148         18        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.0s0.2s\n",
      "                   all        576        400      0.931      0.933      0.954      0.767\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/30      3.86G     0.5904     0.3498      1.123         17        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.5s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.5it/s 2.0s0.2s\n",
      "                   all        576        400       0.95      0.907      0.952       0.76\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/30      3.86G     0.5668     0.3383      1.116         15        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.6it/s 2.0s0.2s\n",
      "                   all        576        400      0.952      0.921      0.952      0.767\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/30      3.86G      0.546     0.3244      1.099         15        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.3s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.5it/s 2.0s0.2s\n",
      "                   all        576        400      0.953      0.904      0.956      0.772\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/30      3.86G     0.5319     0.3142      1.086         14        640: 100% ━━━━━━━━━━━━ 202/202 4.9it/s 41.4s<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 4.4it/s 2.0s0.2s\n",
      "                   all        576        400      0.953      0.917      0.956      0.776\n",
      "\n",
      "30 epochs completed in 0.365 hours.\n",
      "Optimizer stripped from runs\\detect\\train43\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train43\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train43\\weights\\best.pt...\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 9/9 3.6it/s 2.5s0.3s\n",
      "                   all        576        400      0.953      0.917      0.956      0.775\n",
      "                 Paper        132        139      0.962      0.922      0.966      0.771\n",
      "                  Rock        121        141       0.95       0.94       0.95       0.77\n",
      "              Scissors        116        120      0.947       0.89      0.952      0.783\n",
      "Speed: 0.2ms preprocess, 1.6ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train43\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0, 1, 2])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x00000240B62319C0>\n",
       "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
       "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,    0.026446,    0.013223,           0],\n",
       "       [          1,           1,           1, ...,    0.061117,    0.030559,           0],\n",
       "       [          1,           1,           1, ...,    0.024024,    0.012012,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.63658,     0.63658,     0.71449, ...,           0,           0,           0],\n",
       "       [    0.60173,     0.60173,     0.68101, ...,           0,           0,           0],\n",
       "       [    0.56585,     0.56585,     0.63436, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.47518,     0.47518,     0.56757, ...,           1,           1,           1],\n",
       "       [    0.43302,     0.43302,     0.52017, ...,           1,           1,           1],\n",
       "       [        0.4,         0.4,     0.47409, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
       "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
       "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
       "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
       "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
       "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
       "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
       "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
       "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
       "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
       "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
       "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
       "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
       "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
       "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
       "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
       "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
       "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
       "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
       "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
       "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
       "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
       "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
       "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
       "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
       "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
       "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
       "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
       "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
       "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
       "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
       "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
       "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
       "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
       "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
       "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
       "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
       "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
       "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
       "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
       "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
       "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.96403,     0.96403,     0.96403, ...,           0,           0,           0],\n",
       "       [    0.98582,     0.98582,     0.98582, ...,           0,           0,           0],\n",
       "       [    0.96667,     0.96667,     0.95833, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
       "fitness: np.float64(0.7929320708844367)\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
       "maps: array([    0.77143,     0.77036,     0.78271])\n",
       "names: {0: 'Paper', 1: 'Rock', 2: 'Scissors'}\n",
       "nt_per_class: array([139, 141, 120])\n",
       "nt_per_image: array([132, 121, 116])\n",
       "results_dict: {'metrics/precision(B)': 0.953029462809095, 'metrics/recall(B)': 0.9172656317836368, 'metrics/mAP50(B)': 0.9558248089786842, 'metrics/mAP50-95(B)': 0.7748328777628536, 'fitness': 0.7929320708844367}\n",
       "save_dir: WindowsPath('runs/detect/train43')\n",
       "speed: {'preprocess': 0.21283611118229195, 'inference': 1.5918590277882079, 'loss': 0.0008899305107156074, 'postprocess': 0.8557944444570845}\n",
       "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
       "task: 'detect'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "data_path = \"./datasets/data.yaml\"\n",
    "\n",
    "# 튜닝 결과 하이퍼파라미터 파일 불러오기\n",
    "with open(\"runs/detect/tune3/best_hyperparameters.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hyp_dict = yaml.safe_load(f)\n",
    "\n",
    "# 최적 하이퍼파라미터로 학습\n",
    "model.train(\n",
    "    data=data_path,\n",
    "    epochs=30,  \n",
    "    batch=32,\n",
    "    patience=20,\n",
    "    **hyp_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa48b409",
   "metadata": {},
   "source": [
    "| 항목                  | YOLOv8 기본 모델 (train17) | 하이퍼파라미터 적용 (train43) |\n",
    "|-----------------------|----------------------------|-------------------------------|\n",
    "| Precision (정밀도)    | 0.8392                     | **0.9530**                    |\n",
    "| Recall (재현율)       | 0.7423                     | **0.9173**                    |\n",
    "| mAP@50                | 0.7798                     | **0.9558**                    |\n",
    "| mAP@50-95             | 0.4586                     | **0.7748**                    |\n",
    "| Fitness (종합지표)    | 0.4907                     | **0.7929**                    |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda_yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
